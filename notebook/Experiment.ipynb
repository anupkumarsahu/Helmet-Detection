{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ae4d66-d9d7-4f8e-b492-4a40c21e8471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9fc26-362d-4480-b9b5-6944204671f6",
   "metadata": {},
   "source": [
    "Here are all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c37c9f-dd55-4e3b-a5b1-8a4e11dad920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A  # our data augmentation library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7440cb-1cd3-4bfa-be3b-16cc23d43414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove arnings (optional)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bee6271-552c-4c45-af90-78c4aeffb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "0.15.2+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7c2c7a-1430-4fdc-8399-d73b5573500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset is in cocoformat, we will need pypcoco tools\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc14714c-5acb-4376-b8da-19e8029c58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will define our transforms\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d74485-b24a-4b8b-877e-1fd1e902b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(600, 600), # our input size can be 600px\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(600, 600), # our input size can be 600px\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2eb46a-3156-4aed-b86f-1e207b57a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelmetDetection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        # the 3 transform parameters are reuqired for datasets.VisionDataset\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split #train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "    \n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target] # required annotation format for albumentations\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "        \n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "        \n",
    "        new_boxes = [] # convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "        \n",
    "        targ = {} # here is our transformed target\n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ # scale images\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021dac4e-70c4-4fae-8f07-9b889fcf3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\Bappy\\Office\\Projects\\helmet-detection-pytorch\\artifacts\\01_16_2023_19_25_04\\DataIngestionArtifacts\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
